I",<p>Matrices are a representation of <strong>linear maps</strong> between finite-dimensional vector spaces. Consider a  linear map \(\mathcal{L}\) from \(\mathbb{R}^m\) to \(\mathbb{R}^n\).</p>

\[\mathcal{L}: \mathbb{R}^m \mapsto \mathbb{R}^n\]

<p>This linear map can be represented using a real matrix \(\mathbf{A} \in \mathbb{R}^{n \times m}\), such that</p>

\[\mathbf{y} = \mathcal{L}\left( \mathbf{x} \right) = \mathbf{A}\mathbf{x}, \quad \mathbf{x} \in \mathbb{R}^m, \, \mathbf{y} \in \mathbb{R}^n\]

<p>When the domain and range spaces are the same, then \(\mathbf{A}\) is a square matrix.</p>

<p>The matrix \(\mathbf{A}\) has a unique inverse \(\mathbf{A}^{-1}\), if and only if the \(\mathbf{A}\) is full-rank, i.e. the columns of the matrix form a basis for \(\mathbb{R}^n\). In this case, \(\mathbf{A}^{-1}\mathbf{A} = \mathbf{A}\mathbf{A}^{-1} = \mathbf{I}_n\), where \(\mathbf{I}_n\) is the \(n \times n\) identity matrix.</p>

<h3 id="matrix-inverses-allow-us-to-change-basis">Matrix inverses allow us to change basis</h3>

<p>Consider the equation, \(\mathbf{A}\mathbf{x} = \mathbf{y}\). This equation says that the vector \(\mathbf{y}\) is a linear combination of the columns of \(\mathbf{A}\), with the weight for each columns given by the elements of \(\mathbf{x}\).</p>

\[\mathbf{y} = \mathbf{A}\mathbf{x} = \begin{bmatrix} \mathbf{a}_1 &amp; \mathbf{a}_2 &amp; \cdots &amp; \mathbf{a}_n\end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{bmatrix} = \sum_{i=1}^n x_i\mathbf{a}_i\]

<p>Here, \(\mathbf{x}\) is the representation of the vector \(\mathbf{y}\) in the basis of \(\mathbb{R}^n\) formed by the columns of \(\mathbf{A}\) (we will refer to this basis as the <em>column basis</em>). Given \(\mathbf{A}\) and \(\mathbf{y}\), we can solve for \(\mathbf{x}\) using the inverse of \(\mathbf{A}\),</p>

\[\mathbf{x} = \mathbf{A}^{-1}\mathbf{y}\]

<p>Thus, we see that \(\mathbf{A}^{-1}\) is the matrix that allows us to change the representation of a vector to the <em>column basis</em>.</p>

<p><strong>Structure of a matrix inverse</strong>
Let us express $\mathbf{A}^{-1}$ as a columns of rows.</p>

\[\mathbf{A} = \begin{bmatrix} \mathbf{a}_1 &amp; \mathbf{a}_2 &amp; \ldots &amp; \mathbf{a}_n \end{bmatrix} \longrightarrow \mathbf{A}^{-1} = \begin{bmatrix} \mathbf{\tilde{b}}_1^\top \\ \mathbf{\tilde{b}}_2^\top \\ \vdots \\ \mathbf{\tilde{b}}_n^\top\end{bmatrix}\]

<p>Let \(\mathbf{A} = \begin{bmatrix} 1 &amp; 1\\ 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} \mathbf{a}_1 &amp; \mathbf{a}_2 \end{bmatrix}\), then \(\mathbf{A}^{-1} = \begin{bmatrix} 1 &amp; -1 \\ 0 &amp; 1\end{bmatrix} = \begin{bmatrix} \mathbf{\tilde{b}_1^\top} \\ \mathbf{\tilde{b}_2^\top}\end{bmatrix}\). This means that,</p>

\[\mathbf{\tilde{b}}_1^\top \mathbf{a}_1 = 1\]

<p align="center">
<img src="/figs/invbasis.png" width="20%" height="20%" />
</p>

<h3 id="non-square-matrices-have-left-or-right-inverses-but-not-both">Non-square matrices have left or right inverses, but not both</h3>
<p>For full (column or row) rank non-square matrices, we can still have inverses, but there are some peculiarities:</p>
<ol>
  <li>We can only have left or right inverses, as is described below, and (b)</li>
  <li>The left and right inverses are not unique. Let $\mathbf{A} \in \mathbb{R}^{n \times m}$.</li>
</ol>

<h4 id="tall-full-rank-matrix">Tall full rank matrix</h4>

<p>Tall full rank matrices only have left inverses:</p>

\[n &gt; m, \,\, rank\left(\mathbf{A}\right) = m \implies \mathbf{B} \mathbf{A} = \mathbf{I}_{m}\]

<p>This means that, $\mathbf{B}\mathbf{A}\mathbf{x} = \mathbf{B}\mathbf{b} \implies \mathbf{x} = \mathbf{B}\mathbf{b}$.</p>

<p>What $\mathbf{x}$ represents depends on whether or not $\mathbf{b}$ is in the column space of $\mathbf{A}$.</p>

<ul>
  <li>$\mathbf{b} \in \mathcal{C}\left( \mathbf{A} \right) \implies $ $\mathbf{x} = \mathbf{B}\mathbf{b}$ is representation of $\mathbf{b}$ in the <em>column basis</em> of $\mathbf{A}$.</li>
  <li>$\mathbf{b} \notin \mathcal{C}\left( \mathbf{A} \right) \implies $  $\mathbf{x} = \mathbf{Bb}$ is the representation of some vector $\hat{\mathbf{b}}$ $=$ $\mathbf{ABb}$ in the <em>column basis</em> of $\mathbf{A}$.</li>
</ul>

<p>When $\mathbf{x}$ is substituted back into the original equation, we get</p>

\[\begin{cases}
\mathbf{b} \in \mathcal{C}\left(\mathbf{A}\right) &amp;\implies \mathbf{A}\left(\mathbf{B}\mathbf{b}\right) = \mathbf{b} \\
\mathbf{b} \notin \mathcal{C}\left(\mathbf{A}\right) &amp;\implies \mathbf{A}\left(\mathbf{B}\mathbf{b}\right) \neq \mathbf{b}
\end{cases}\]

<p>For a given left inverse $\mathbf{B}$, adding a matrix $\mathbf{C}$ whose rows are orthogonal to the columns of $\mathbf{A}$ will result in another left inverse of $\mathbf{A}$.</p>

\[\left( \mathbf{B} + \mathbf{C} \right) \mathbf{A} = \mathbf{I}_m, \, s.t. \, \mathbf{C}\mathbf{A} = \mathbf{0}\]

<p>The rows of $\mathbf{C}$ will be vectors from $\mathcal{N}\left(\mathbf{A}^\top\right)$ - the left nullspace of $\mathbf{A}$. Thus, it is clear that there are infinitely many left inverses for $\mathbf{A}$.</p>

<p><strong>What do all these left inverse matrices do?</strong></p>

<blockquote>
  <p>A left inverse allows us to find the representation of a component of the vector $\mathbf{b}$ in $\mathcal{C}\left(\mathbf{A}\right)$ represented in the the basis formed by the columns of $\mathbf{A}$.</p>
</blockquote>

<h4 id="fat-full-rank-matrix">Fat full rank matrix</h4>

\[n &lt; m, \,\, rank\left(\mathbf{A}\right) = n \implies \mathbf{A} \mathbf{B} = \mathbf{I}_{n}\]

<ul>
  <li>The left and the right inverses are not unique, there an infinite number of left and right inverses.</li>
</ul>

:ET