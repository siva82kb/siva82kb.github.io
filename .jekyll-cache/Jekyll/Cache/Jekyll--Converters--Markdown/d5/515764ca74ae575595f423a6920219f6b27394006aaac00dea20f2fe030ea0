I"Ø<p>\(\newcommand{\mf}{\mathbf} \newcommand{\mb}{\mathbb} \newcommand{\mc}{\mathcal}\)
Fat full rank matrices only have right inverses:</p>

\[n &lt; m, \,\, rank\left(\mf{A}\right) = n \implies \exists \mf{B} \in \mb{R}^{m \times n} \,\, s.t. \,\, \mf{A} \mf{B} = \mf{I}_{n}\]

<p>This means that, \(\mf{A}\mf{B}\mf{y} = \mf{y}\).</p>

<p>For full rank fat matrices, the four fundamnetal subspaces have the following dimensions.</p>

\[\begin{split} \text{dim } \mc{C}\left( \mf{A}^\top \right) = n &amp; \text{dim } \mc{N}\left( \mf{A} \right) = m-n \\
\text{dim } \mc{C}\left( \mf{A} \right) = n &amp; \text{dim } \mc{N}\left( \mf{A}^\top \right) = 0
\end{split}\]

<p>What $\mf{x}$ represents depends on whether or not $\mf{y}$ is in the column space of $\mf{A}$.</p>

<ul>
  <li>$\mf{y} \in \mc{C}\left( \mf{A} \right) \implies $ $\mf{x} = \mf{B}\mf{y}$ is the representation of $\mf{y}$ in the <em>column basis</em> of $\mf{A}$.</li>
  <li>$\mf{y} \notin \mc{C}\left( \mf{A} \right) \implies $  $\mf{x} = \mf{Bb}$ is the representation of some vector $\hat{\mf{y}}$ $=$ $\mf{A}\mf{x} = \mf{ABb}$, which in the <em>column basis</em> of $\mf{A}$.</li>
</ul>

<p>When $\mf{x}$ is substituted back into the original equation, we get</p>

\[\begin{cases}
\mf{y} \in \mc{C}\left(\mf{A}\right) &amp;\implies \hat{\mf{y}} = \mf{A}\left(\mf{B}\mf{y}\right) = \mf{y} \\
\mf{y} \notin \mc{C}\left(\mf{A}\right) &amp;\implies \hat{\mf{y}} = \mf{A}\left(\mf{B}\mf{y}\right) \neq \mf{y}
\end{cases}\]

<p>For a given left inverse $\mf{B}$, adding a matrix $\mf{C}$ whose rows are orthogonal to the columns of $\mf{A}$ will result in another left inverse of $\mf{A}$.</p>

\[\left( \mf{B} + \mf{C} \right) \mf{A} = \mf{I}_m, \, s.t. \, \mf{C}\mf{A} = \mf{0}\]

<p>The rows of $\mf{C}$ will be vectors from $\mc{N}\left(\mf{A}^\top\right)$ - the left nullspace of $\mf{A}$. Thus, it is clear that there are infinitely many left inverses for $\mf{A}$.</p>

<p><strong>What do all these left inverse matrices do?</strong></p>

<blockquote>
  <p>A left inverse allows us to find the representation of \(\hat{\mf{y}}\) â€“ a component of \(\mf{y}\) in \(\mc{C}\left(\mf{A}\right)\) â€“ in the <em>column basis</em> of \(\mf{A}\).</p>
</blockquote>

<p>These components are oblique projections onto \(\mc{C}\left(\mf{A}\right)\) along a complementary subspace to \(\mc{C}\left(\mf{A}\right)\) in \(\mb{R}^n\). The matrix \(\mf{A}\left( \mf{B} + \mf{C}\right)\) is this oblique projection matrix.</p>

\[\hat{\mf{y}} = \mf{A}\left(\mf{B} + \mf{C}\right) \mf{y}\]

<p>In the absence of a mathetical proof of this, letâ€™s convince ourselves that this is true through a simple example. Let \(\mf{A} = \begin{bmatrix} 1 \\ 1\end{bmatrix}\). The following are the various possible left inverses of \(\mf{A}\),</p>

\[\mf{B} + \mf{C} = \begin{bmatrix} \mf{\tilde{b}}_1^\top \end{bmatrix} + \begin{bmatrix} \mf{\tilde{c}}_1^\top \end{bmatrix} = \frac{1}{2}\begin{bmatrix} 1 &amp; 1 \end{bmatrix} + \begin{bmatrix} \alpha &amp; -\alpha \end{bmatrix}\]

<p>The following figures depicts this particular example,</p>

<p><img src="/figs/leftinv1.png" alt="" style="float: left; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 300px" /></p>

<p>In this figure, we have assumed \(\alpha = -1 \implies \mf{\tilde{c}}_1 = \begin{bmatrix} -1 \\ 1\end{bmatrix}\), and \(\mf{y} = \begin{bmatrix} 1 \\ 2\end{bmatrix}\). Thus,</p>

\[\begin{split}
\hat{\mf{y}} &amp;= \mf{A} \left( \mf{B} + \mf{C}\right) \mf{y} \\
             &amp;= \begin{bmatrix} 1 \\ 1\end{bmatrix} \begin{bmatrix} -\frac{1}{2}  &amp; \frac{3}{2}\end{bmatrix} \begin{bmatrix} 1 \\ 2\end{bmatrix} \\
             &amp;= \begin{bmatrix} \frac{5}{2} \\ \frac{5}{2} \end{bmatrix}
             \end{split}\]

<p>\(\hat{\mf{y}}\) is the oblique projection of \(\mf{y}\) onto \(\mc{C}\left(\mf{A}\right)\) along the subspace parallel to the dashed black line joining \(\mf{y}\) and \(\hat{\mf{y}}\) in the above figure. This is the subspace spanned by the vector \(\mf{y} - \hat{\mf{y}} = \begin{bmatrix} \frac{3}{2} \\ \frac{1}{2} \end{bmatrix}\). It is important to that \(\mf{\tilde{b}}_1 + \mf{\tilde{c}}_1\) is orthogonal to \(\mf{y} - \hat{\mf{y}}\).</p>

\[\left( \mf{\tilde{b}}_1 + \mf{\tilde{c}}_1\right)^\top\left( \mf{y} - \hat{\mf{y}} \right) = 0\]

<p>The following examples demonstrates the same for a different value of \(\alpha = 2.5\).</p>

<p><img src="/figs/leftinv2.png" alt="" style="align: center; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 400px" /></p>

<p>It is left as an exercise to work out the different components and verfiy that \(\left( \mf{\tilde{b}}_1 + \mf{\tilde{c}}_1\right) \perp \left(\mf{y} - \hat{\mf{y}}\right)\).</p>

<p>For any choice of \(\alpha\) we can get a oblique project matrix onto \(\mc{C}\left(\mf{A}\right)\) along a subspace orthogonal to that of \(\mc{C}\left(\mf{B}^\top + \mf{C}^\top\right)\), i.e. the row space of the left inverse.</p>

<p><strong>A special left inverse.</strong></p>

<p>Among the infintely many oblique projectors, the orthogonal projector is a speical one. The left inverse of \(\mf{A}\) that will allow us to do orthogonal projection is the <em>Moore-Penrose Inverse</em> or the <em>Pseudoinverse</em>, expressed as \(\mf{A}^\dagger\).</p>

\[\mf{A}^\dagger = \left(\mf{A}^\top\mf{A}\right)^{-1}\mf{A}^\top\]

<p>The matrix \(\mf{B} = \begin{bmatrix} \mf{\tilde{b}}_1^\top \end{bmatrix}  = \frac{1}{2}\begin{bmatrix} 1 &amp; 1 \end{bmatrix}\) shown in the above figures is the <em>pseudoinverse</em> of \(\mf{A}\).</p>

<p><img src="/figs/left_pinv.png" alt="" style="align: center; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 600px" /></p>

<p>The mapping performed by the psuedoinverse \(\mathbf{A}^\dagger\) is shown in the above figure.</p>

<p>How do know that this is in fact a orthogonal projector?</p>

<p>\(\mc{C}\left(\left(\mf{A}^\dagger\right)^\top\right) = \mc{C}\left(\mf{A}\right)\), which means that the matrix \(\mf{A}\mf{A}^\dagger\) is the projection matrix onto \(\mc{C}\left( \mf{A} \right)\) along the subspace orthogonal to \(\mc{C}\left(\mf{A}\right)\), which is \(\mc{N}\left(\mf{A}^\top\right)\).</p>

<p>I hope this short presentaion helps shed some light on the geometry operation behind left inverses. The next part of this series will focus on the right inverse.</p>
:ET