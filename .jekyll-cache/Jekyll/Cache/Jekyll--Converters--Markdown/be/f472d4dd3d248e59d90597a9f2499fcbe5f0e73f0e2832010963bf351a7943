I"K<p>\(\newcommand{\mf}{\mathbf} \newcommand{\mb}{\mathbb} \newcommand{\mc}{\mathcal}\)
For full (column or row) rank non-square matrices \(\mf{A} \in \mb{R}^{n \times m}\), we can still have inverses but there are some peculiarities:</p>
<ol>
  <li>We can only have left or right inverses, as is described below, and</li>
  <li>The left and right inverses are not unique.</li>
</ol>

<p>We will talk about the inverses of tall matrices in this post, i.e. matrices where $n &gt; m$. Tall full rank matrices only have left inverses:</p>

\[n &gt; m, \,\, rank\left(\mf{A}\right) = m \implies \exists \mf{B} \in \mb{R}^{m \times n} \,\, s.t. \,\, \mf{B} \mf{A} = \mf{I}_{m}\]

<p>This means that, $\mf{B}\mf{A}\mf{x} = \mf{B}\mf{b} \implies \mf{x} = \mf{B}\mf{b}$.</p>

<p>What $\mf{x}$ represents depends on whether or not $\mf{b}$ is in the column space of $\mf{A}$.</p>

<ul>
  <li>$\mf{b} \in \mc{C}\left( \mf{A} \right) \implies $ $\mf{x} = \mf{B}\mf{b}$ is the representation of $\mf{b}$ in the <em>column basis</em> of $\mf{A}$.</li>
  <li>$\mf{b} \notin \mc{C}\left( \mf{A} \right) \implies $  $\mf{x} = \mf{Bb}$ is the representation of some vector $\hat{\mf{b}}$ $=$ $\mf{A}\mf{x} = \mf{ABb}$, which in the <em>column basis</em> of $\mf{A}$.</li>
</ul>

<p>When $\mf{x}$ is substituted back into the original equation, we get</p>

\[\begin{cases}
\mf{b} \in \mc{C}\left(\mf{A}\right) &amp;\implies \hat{\mf{b}} = \mf{A}\left(\mf{B}\mf{b}\right) = \mf{b} \\
\mf{b} \notin \mc{C}\left(\mf{A}\right) &amp;\implies \hat{\mf{b}} = \mf{A}\left(\mf{B}\mf{b}\right) \neq \mf{b}
\end{cases}\]

<p>For a given left inverse $\mf{B}$, adding a matrix $\mf{C}$ whose rows are orthogonal to the columns of $\mf{A}$ will result in another left inverse of $\mf{A}$.</p>

\[\left( \mf{B} + \mf{C} \right) \mf{A} = \mf{I}_m, \, s.t. \, \mf{C}\mf{A} = \mf{0}\]

<p>The rows of $\mf{C}$ will be vectors from $\mc{N}\left(\mf{A}^\top\right)$ - the left nullspace of $\mf{A}$. Thus, it is clear that there are infinitely many left inverses for $\mf{A}$.</p>

<p><strong>What do all these left inverse matrices do?</strong></p>

<blockquote>
  <p>A left inverse allows us to find the representation of \(\hat{\mf{b}}\) – a component of \(\mf{b}\) in \(\mc{C}\left(\mf{A}\right)\) – in the <em>column basis</em> of \(\mf{A}\).</p>
</blockquote>

<p>These components are oblique projections onto \(\mc{C}\left(\mf{A}\right)\) along a complementary subspace to \(\mc{C}\left(\mf{A}\right)\) in \(\mb{R}^n\). The matrix \(\mf{A}\left( \mf{B} + \mf{C}\right)\) is this oblique projection matrix.</p>

\[\hat{\mf{b}} = \mf{A}\left(\mf{B} + \mf{C}\right) \mf{b}\]

<p>In the absence of a mathetical proof of this, let’s convince ourselves that this is true through a simple example. Let \(\mf{A} = \begin{bmatrix} 1 \\ 1\end{bmatrix}\). The following are the various possible left inverses of \(\mf{A}\),</p>
:ET