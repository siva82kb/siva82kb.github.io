I"²<p>\(\newcommand{\mf}{\mathbf} \newcommand{\mb}{\mathbb} \newcommand{\mc}{\mathcal}\)
Fat full rank matrices only have right inverses:</p>

\[n &lt; m, \,\, rank\left(\mf{A}\right) = n \implies \exists \mf{B} \in \mb{R}^{m \times n} \,\, s.t. \,\, \mf{A} \mf{B} = \mf{I}_{n}\]

<p>This means that, \(\mf{A}\mf{B}\mf{y} = \mf{y}\).</p>

<p>For full rank fat matrices, the four fundamnetal subspaces have the following dimensions.</p>

\[\begin{split} \text{dim } \mc{C}\left( \mf{A}^\top \right) = n &amp; \quad \quad \quad \text{dim } \mc{N}\left( \mf{A} \right) = m-n \\
\text{dim } \mc{C}\left( \mf{A} \right) = n &amp; \quad \quad \quad \text{dim } \mc{N}\left( \mf{A}^\top \right) = 0
\end{split}\]

<p>This means that the general solution to the equation, \(\mf{A}\mf{x} = \mf{y}\) has infinitely many solutions of the form,</p>

\[\mf{x} = \mf{x}_{rs} + \mf{x}_{ns}\]

<p>where, \(\mf{x}_{rs} \in \mc{C}\left( \mf{A}^\top\right)\) is the row space component of the solution, and \(\mf{x}_{ns} \in \mc{N}\left( \mf{A} \right)\) is the nullspace component, such that</p>

\[\mf{A}\mf{x}_{rs} = \mf{y} \quad \text{and} \quad \mf{A}\mf{x}_{ns} = \mf{0}\]

<p>This means that entire hyperplanes of dimension \(m-n\) gets mapped to a single point \(\mf{y}\).</p>

<p>Even though the mapping from \(\mb{R}^m\) to \(\mb{R}^n\) carried out by \(\mf{A}\) is not unique, <strong>the mapping from the row space to the column space of  \(\mf{A}\) is unique.</strong></p>

<p><strong>Right inverses are not unqiue</strong></p>

<p>If \(\mf{B}\) is a right inverse of \(\mf{A}\), then any matrix of the form \(\mf{B} + \mf{C}\) will also be left inverse, if the columns of the matrix \(\mf{C}\) are from the \(\mc{N}\left(\mf{A}\right)\), i.e. \(\mc{C}\left( \mf{C} \right) \subset \mc{N}\left( \mf{A} \right) \implies \mf{A}\mf{C} = \mf{0}\).</p>

<p><strong>What do all these right inverse matrices do?</strong></p>

<blockquote>
  <p>A right inverse allows us to find one of the possible solutions to the equation \(\mf{A}\mf{x} = \mf{y}\).</p>
</blockquote>

<p>The set of all right inverses when post multiplied by \(\mf{y}\) will produce the entire list of all solutions to the equation \(\mf{A}\mf{x} = \mf{y}\).</p>

<p>Any vector of the form \(\mf{x} = \left(\mf{B} + \mf{C}\right)\mf{y} \in \mb{R}^m\) will have both components from the row space and nullspace. Letâ€™s assume that the \(\mf{x}_{rs} = \mf{B}\mf{y}\) produces the row space component, and \(\mf{x}_{ns} = \mf{C}\mf{y}\) produces the nullspace component.</p>

<p>Producing the nullspace component is easy. We simply build a matrix using vectors from \(\mc{N}\left(\mf{A}\right)\), such that \(\mf{C}\mf{y}\) produces components from \(\mc{N}\left(\mf{A}\right)\).</p>

<!-- What $\mf{x}$ represents depends on whether or not $\mf{y}$ is in the column space of $\mf{A}$. 

- $\mf{y} \in \mc{C}\left( \mf{A} \right) \implies $ $\mf{x} = \mf{B}\mf{y}$ is the representation of $\mf{y}$ in the *column basis* of $\mf{A}$.
- $\mf{y} \notin \mc{C}\left( \mf{A} \right) \implies $  $\mf{x} = \mf{Bb}$ is the representation of some vector $\hat{\mf{y}}$ $=$ $\mf{A}\mf{x} = \mf{ABb}$, which in the *column basis* of $\mf{A}$.

When $\mf{x}$ is substituted back into the original equation, we get

$$ \begin{cases}
\mf{y} \in \mc{C}\left(\mf{A}\right) &\implies \hat{\mf{y}} = \mf{A}\left(\mf{B}\mf{y}\right) = \mf{y} \\
\mf{y} \notin \mc{C}\left(\mf{A}\right) &\implies \hat{\mf{y}} = \mf{A}\left(\mf{B}\mf{y}\right) \neq \mf{y}
\end{cases}
$$

For a given left inverse $\mf{B}$, adding a matrix $\mf{C}$ whose rows are orthogonal to the columns of $\mf{A}$ will result in another left inverse of $\mf{A}$.

$$ \left( \mf{B} + \mf{C} \right) \mf{A} = \mf{I}_m, \, s.t. \, \mf{C}\mf{A} = \mf{0} $$

The rows of $\mf{C}$ will be vectors from $\mc{N}\left(\mf{A}^\top\right)$ - the left nullspace of $\mf{A}$. Thus, it is clear that there are infinitely many left inverses for $\mf{A}$.

**What do all these left inverse matrices do?**

> A left inverse allows us to find the representation of $$\hat{\mf{y}}$$ -- a component of $$\mf{y}$$ in $$\mc{C}\left(\mf{A}\right)$$ -- in the *column basis* of $$\mf{A}$$.

These components are oblique projections onto $$\mc{C}\left(\mf{A}\right)$$ along a complementary subspace to $$\mc{C}\left(\mf{A}\right)$$ in $$\mb{R}^n$$. The matrix $$\mf{A}\left( \mf{B} + \mf{C}\right)$$ is this oblique projection matrix.

$$ \hat{\mf{y}} = \mf{A}\left(\mf{B} + \mf{C}\right) \mf{y}$$

In the absence of a mathetical proof of this, let's convince ourselves that this is true through a simple example. Let $$\mf{A} = \begin{bmatrix} 1 \\ 1\end{bmatrix}$$. The following are the various possible left inverses of $$\mf{A}$$,

$$ \mf{B} + \mf{C} = \begin{bmatrix} \mf{\tilde{b}}_1^\top \end{bmatrix} + \begin{bmatrix} \mf{\tilde{c}}_1^\top \end{bmatrix} = \frac{1}{2}\begin{bmatrix} 1 & 1 \end{bmatrix} + \begin{bmatrix} \alpha & -\alpha \end{bmatrix}$$

The following figures depicts this particular example, -->

<!-- ![](/figs/leftinv1.png){:style="float: left; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 300px"}

In this figure, we have assumed $$\alpha = -1 \implies \mf{\tilde{c}}_1 = \begin{bmatrix} -1 \\ 1\end{bmatrix}$$, and $$\mf{y} = \begin{bmatrix} 1 \\ 2\end{bmatrix}$$. Thus, 

$$ \begin{split}
\hat{\mf{y}} &= \mf{A} \left( \mf{B} + \mf{C}\right) \mf{y} \\
             &= \begin{bmatrix} 1 \\ 1\end{bmatrix} \begin{bmatrix} -\frac{1}{2}  & \frac{3}{2}\end{bmatrix} \begin{bmatrix} 1 \\ 2\end{bmatrix} \\
             &= \begin{bmatrix} \frac{5}{2} \\ \frac{5}{2} \end{bmatrix}
             \end{split}$$

$$\hat{\mf{y}}$$ is the oblique projection of $$\mf{y}$$ onto $$\mc{C}\left(\mf{A}\right)$$ along the subspace parallel to the dashed black line joining $$\mf{y}$$ and $$\hat{\mf{y}}$$ in the above figure. This is the subspace spanned by the vector $$\mf{y} - \hat{\mf{y}} = \begin{bmatrix} \frac{3}{2} \\ \frac{1}{2} \end{bmatrix}$$. It is important to that $$\mf{\tilde{b}}_1 + \mf{\tilde{c}}_1$$ is orthogonal to $$\mf{y} - \hat{\mf{y}}$$.

$$ \left( \mf{\tilde{b}}_1 + \mf{\tilde{c}}_1\right)^\top\left( \mf{y} - \hat{\mf{y}} \right) = 0$$

The following examples demonstrates the same for a different value of $$\alpha = 2.5$$.

![](/figs/leftinv2.png){:style="align: center; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 400px"}

It is left as an exercise to work out the different components and verfiy that $$\left( \mf{\tilde{b}}_1 + \mf{\tilde{c}}_1\right) \perp \left(\mf{y} - \hat{\mf{y}}\right)$$.

For any choice of $$\alpha$$ we can get a oblique project matrix onto $$\mc{C}\left(\mf{A}\right)$$ along a subspace orthogonal to that of $$\mc{C}\left(\mf{B}^\top + \mf{C}^\top\right)$$, i.e. the row space of the left inverse.

**A special left inverse.**

Among the infintely many oblique projectors, the orthogonal projector is a speical one. The left inverse of $$\mf{A}$$ that will allow us to do orthogonal projection is the *Moore-Penrose Inverse* or the *Pseudoinverse*, expressed as $$\mf{A}^\dagger$$.

$$ \mf{A}^\dagger = \left(\mf{A}^\top\mf{A}\right)^{-1}\mf{A}^\top$$ 

The matrix $$\mf{B} = \begin{bmatrix} \mf{\tilde{b}}_1^\top \end{bmatrix}  = \frac{1}{2}\begin{bmatrix} 1 & 1 \end{bmatrix}$$ shown in the above figures is the *pseudoinverse* of $$\mf{A}$$. -->

<p><img src="/figs/right_pinv.png" alt="" style="align: center; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 600px" /></p>

<!-- The mapping performed by the psuedoinverse $$\mathbf{A}^\dagger$$ is shown in the above figure.

How do know that this is in fact a orthogonal projector?

$$\mc{C}\left(\left(\mf{A}^\dagger\right)^\top\right) = \mc{C}\left(\mf{A}\right)$$, which means that the matrix $$\mf{A}\mf{A}^\dagger$$ is the projection matrix onto $$\mc{C}\left( \mf{A} \right)$$ along the subspace orthogonal to $$\mc{C}\left(\mf{A}\right)$$, which is $$\mc{N}\left(\mf{A}^\top\right)$$.

I hope this short presentaion helps shed some light on the geometry operation behind left inverses. The next part of this series will focus on the right inverse. -->
:ET