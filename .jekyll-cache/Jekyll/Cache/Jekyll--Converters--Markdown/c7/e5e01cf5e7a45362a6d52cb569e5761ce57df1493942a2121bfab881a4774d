I"°<p>\(\newcommand{\mf}{\mathbf} \newcommand{\mb}{\mathbb} \newcommand{\mc}{\mathcal}\)
For full (column or row) rank non-square matrices \(\mf{A} \in \mb{R}^{n \times m}\), we can still have inverses but there are some peculiarities:</p>
<ol>
  <li>We can only have left or right inverses, as is described below, and</li>
  <li>The left and right inverses are not unique.</li>
</ol>

<p>We will talk about the inverses of tall matrices in this post, i.e. matrices where $n &gt; m$. Tall full rank matrices only have left inverses:</p>

\[n &gt; m, \,\, rank\left(\mf{A}\right) = m \implies \exists \mf{B} \in \mb{R}^{m \times n} \,\, s.t. \,\, \mf{B} \mf{A} = \mf{I}_{m}\]

<p>This means that, $\mf{B}\mf{A}\mf{x} = \mf{B}\mf{b} \implies \mf{x} = \mf{B}\mf{b}$.</p>

<p>What $\mf{x}$ represents depends on whether or not $\mf{b}$ is in the column space of $\mf{A}$.</p>

<ul>
  <li>$\mf{b} \in \mc{C}\left( \mf{A} \right) \implies $ $\mf{x} = \mf{B}\mf{b}$ is the representation of $\mf{b}$ in the <em>column basis</em> of $\mf{A}$.</li>
  <li>$\mf{b} \notin \mc{C}\left( \mf{A} \right) \implies $  $\mf{x} = \mf{Bb}$ is the representation of some vector $\hat{\mf{b}}$ $=$ $\mf{A}\mf{x} = \mf{ABb}$, which in the <em>column basis</em> of $\mf{A}$.</li>
</ul>

<p>When $\mf{x}$ is substituted back into the original equation, we get</p>

\[\begin{cases}
\mf{b} \in \mc{C}\left(\mf{A}\right) &amp;\implies \hat{\mf{b}} = \mf{A}\left(\mf{B}\mf{b}\right) = \mf{b} \\
\mf{b} \notin \mc{C}\left(\mf{A}\right) &amp;\implies \hat{\mf{b}} = \mf{A}\left(\mf{B}\mf{b}\right) \neq \mf{b}
\end{cases}\]

<p>For a given left inverse $\mf{B}$, adding a matrix $\mf{C}$ whose rows are orthogonal to the columns of $\mf{A}$ will result in another left inverse of $\mf{A}$.</p>

\[\left( \mf{B} + \mf{C} \right) \mf{A} = \mf{I}_m, \, s.t. \, \mf{C}\mf{A} = \mf{0}\]

<p>The rows of $\mf{C}$ will be vectors from $\mc{N}\left(\mf{A}^\top\right)$ - the left nullspace of $\mf{A}$. Thus, it is clear that there are infinitely many left inverses for $\mf{A}$.</p>

<p><strong>What do all these left inverse matrices do?</strong></p>

<blockquote>
  <p>A left inverse allows us to find the representation of \(\hat{\mf{b}}\) â€“ a component of \(\mf{b}\) in \(\mc{C}\left(\mf{A}\right)\) â€“ in the <em>column basis</em> of \(\mf{A}\).</p>
</blockquote>

<p>These components are oblique projections onto \(\mc{C}\left(\mf{A}\right)\) along a complementary subspace to \(\mc{C}\left(\mf{A}\right)\) in \(\mb{R}^n\). The matrix \(\mf{A}\left( \mf{B} + \mf{C}\right)\) is this oblique projection matrix.</p>

\[\hat{\mf{b}} = \mf{A}\left(\mf{B} + \mf{C}\right) \mf{b}\]

<p>In the absence of a mathetical proof of this, letâ€™s convince ourselves that this is true through a simple example. Let \(\mf{A} = \begin{bmatrix} 1 \\ 1\end{bmatrix}\). The following are the various possible left inverses of \(\mf{A}\),</p>

\[\mf{B} + \mf{C} = \begin{bmatrix} \mf{\tilde{b}}_1^\top \end{bmatrix} + \begin{bmatrix} \mf{\tilde{c}}_1^\top \end{bmatrix} =**** \frac{1}{2}\begin{bmatrix} 1 &amp; 1 \end{bmatrix} + \begin{bmatrix} \alpha &amp; -\alpha \end{bmatrix}\]

<p>The following figures depicts this particular example,</p>

<p><img src="/figs/leftinv1.png" alt="" style="float: left; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 300px" /></p>

<p>In this figure, we have assumed \(\alpha = -1 \implies \mf{\tilde{c}}_1 = \begin{bmatrix} -1 \\ 1\end{bmatrix}\), and \(\mf{y} = \begin{bmatrix} 1 \\ 2\end{bmatrix}\). Thus,</p>

\[\begin{split}
\hat{\mf{y}} &amp;= \mf{A} \left( \mf{B} + \mf{C}\right) \mf{y} \\
             &amp;= \begin{bmatrix} 1 \\ 1\end{bmatrix} \begin{bmatrix} -\frac{1}{2}  &amp; \frac{3}{2}\end{bmatrix} \begin{bmatrix} 1 \\ 2\end{bmatrix} \\
             &amp;= \begin{bmatrix} \frac{5}{2} \\ \frac{5}{2} \end{bmatrix}
             \end{split}\]

<p>\(\hat{\mf{y}}\) is the oblique projection of \(\mf{y}\) onto \(\mc{C}\left(\mf{A}\right)\) along the subspace parallel to the dashed black line joining \(\mf{y}\) and \(\hat{\mf{y}}\) in the above figure. This is the subspace spanned by the vector \(\mf{y} - \hat{\mf{y}} = \begin{bmatrix} \frac{3}{2} \\ \frac{1}{2} \end{bmatrix}\). It is important to that \(\mf{\tilde{b}}_1 + \mf{\tilde{c}}_1\) is orthogonal to \(\mf{y} - \hat{\mf{y}}\).</p>

\[\left( \mf{\tilde{b}}_1 + \mf{\tilde{c}}_1\right)^\top\left( \mf{y} - \hat{\mf{y}} \right) = 0\]

<p>The following examples demonstrates the same for a different value of \(\alpha = 2.5\).</p>

<p><img src="/figs/leftinv2.png" alt="" style="align: center; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 400px" /></p>

<p>It is left as an exercise to work out the different components and verfiy that \(\left( \mf{\tilde{b}}_1 + \mf{\tilde{c}}_1\right) \perp \left(\mf{y} - \hat{\mf{y}}\right)\).</p>

<p>For any choice of \(\alpha\) we can get a oblique project matrix onto \(\mc{C}\left(\mf{A}\right)\) along a subspace orthogonal to that of \(\mc{C}\left(\mf{B}^\top + \mf{C}^\top\right)\), i.e. the row space of the left inverse.</p>

<p><strong>A special left inverse.</strong></p>

<p>Among the infintely many oblique projectors, the orthogonal projector is a speical one. The left inverse of \(\mf{A}\) that will allow us to do orthogonal projection is the <em>Moore-Penrose Inverse</em> or the <em>Pseudoinverse</em>, expressed as \(\mf{A}^\dagger\).</p>

\[\mf{A}^\dagger = \left(\mf{A}^\top\mf{A}\right)^{-1}\mf{A}^\top\]

<p>The matrxi \(\mf{B} = \begin{bmatrix} \mf{\tilde{b}}_1^\top \end{bmatrix}  = \frac{1}{2}\begin{bmatrix} 1 &amp; 1 \end{bmatrix}\) shown in the above figures is the <em>pseudoinverse</em> of \(\mf{A}\).</p>

<p><img src="/figs/left_-_pinv.png" alt="" style="align: center; margin-left: 5px; margin-right: 10px; margin-top: 7px; width: 500px" /></p>
:ET